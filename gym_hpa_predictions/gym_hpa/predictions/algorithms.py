import numpy as np
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import Dense, Dropout, LSTM
import keras
import plotly.graph_objs as go
import pandas as pd
from pmdarima.arima import auto_arima
from statsmodels.tsa.arima.model import ARIMA
import tensorflow as tf


class Prediction:


    def __init__(self, date_col, val_col, trainset, testset=None, current_value=1, n_steps=1, past_horizon = 60):
        """
        This class contains all the prediction algorithms used in the project, as well as all the functions needed
        to modify the dataset if necessary to make accurate predictions

        :param date_col: The name of the date column
        :param val_col: The name column that you want to predict without the deployment name (e.g. _cpu_usage)
        :param trainset: The directory of the dataset that you will use for training certain models
        :param testset: The directory of the dataset that you will use for testing. If not defined, it is the same as
                        the trainset.
        :param current_value: The value that the metric has right now (probably not saved in the dataset yet)
        :param n_steps: The forecasting horizon
        :param past_horizon: The number of past observations that the model has access to, to make the predictions
        """

        # super(Prediction, self).__init__()
        self.trainset = pd.read_csv(trainset)

        self.testset = pd.read_csv(testset) if testset is not None else  \
            pd.read_csv(trainset)

        self.date_col = date_col
        self.val_col = val_col
        self.current_value = current_value
        self.n_steps = n_steps
        self.past_horizon = past_horizon


    def convert_to_timeseries(self):
        """
         Because the datasets might not be equal spaced (dublicate observations in a second, no observations
         in a second, missing values), this function converts them to a timeseries so that every second there is exactly
         one observation. The missing values are filled as the value of the previous observation.

        :return: the final timeseries
        """
        datasets = [self.trainset, self.testset]

        # Step 0: Drop duplicates
        for dataset in datasets:
            dataset.drop_duplicates(subset=[self.date_col], keep='first', inplace=True)

        df_resampled = []

        for i in range(len(datasets)):
            df = datasets[i].copy()  # Create a copy of the DataFrame to avoid any potential side-effects

            # Step 1: Convert Timestamp column to pandas DateTime type
            df[self.date_col] = pd.to_datetime(df[self.date_col])

            # Step 2: Set the Timestamp column as the index
            df.set_index(self.date_col, inplace=True)

            # Step 3: Resample and create a complete time range with desired frequency (e.g. 1 second)
            frequency = "1S"
            df_resampled.append(df.resample(frequency).asfreq())

            # If you want to fill the missing values with a specific value, you can do the following:
            # value =
            # df_resampled.fillna(value, inplace=True)

            # If you want to fill the missing values using forward fill (carrying the last known value forward):
            df_resampled[i].fillna(method='ffill', inplace=True)

            # If you want to fill the missing values using backward fill (carrying the next known value backward):
            # df_resampled.fillna(method='bfill', inplace=True)

            # Step 4: Reset the index if needed
            df_resampled[i].reset_index(inplace=True)

        return df_resampled

    @staticmethod
    def load_lstm(directory):
        """
        Loads a pre-trained LSTM model.

        :param directory: The directory to load the model from
        :return: The loaded model
        """
        loaded_model = tf.keras.models.load_model(directory)
        return loaded_model

    def create_env_for_NN(self, data, new_value=None):
        """
        Transforms the dataset to a type that can be used to make predictions in the LSTM model.

        :param data: the dataset to be converted
        :param new_value: the latest value, that is not yet saved in the dataset. It is used for the out-of-sample lstm.
        :return: x: the scaled observation space for the lstm (past observations)
                 y: the actual values (used for error correction and learning purposes)
                 scaler: the scaler used to scale the dataset for training. It will then be used to convert the scaled predictions to actual values
        """
        dataset = data.filter([self.val_col]).values

        if new_value is not None:
            a_list = dataset.tolist()

            # Append the new value to the list
            a_list.append([new_value])

            # Convert the list back to a NumPy array
            dataset = np.array(a_list)
            print(dataset)

        env_data_len = int(np.ceil(len(dataset)))

        # Scale our data from 0 to 1
        scaler = MinMaxScaler(feature_range=(0, 1))
        scaled_data = scaler.fit_transform(dataset)

        # Use our scaled data for training
        env_data = scaled_data[0:int(env_data_len), :]
        x = []
        y = []

        #y.append(env_data[0:50, 0])
        for i in range(self.past_horizon, len(env_data)):
            x.append(env_data[i - self.past_horizon:i, 0])
            y.append(env_data[i, 0])

        x, y = np.array(x), np.array(y)

        x = np.reshape(x, (x.shape[0], x.shape[1], 1))

        return x, y, scaler

    @staticmethod
    def lstm_train(x_train, y_train, batch_size, epochs):
        """
        Implements the training of the LSTM algorithm for predictions. It also saves it in the corresponding directory.
        It should be avoided to be run locally and be run in the Colab notebook instead, in order to use the gpu.

        :param x_train: The normalized x values (observations) of the training
        :param y_train: The normalized y values (next observations) of the training
        :param batch_size: The batch size of the training
        :param epochs: The epochs to train
        :return: The trained model
        """

        # Build LSTM model

        model = Sequential()
        model.add(LSTM(128, return_sequences=True, input_shape=(x_train.shape[1], 1)))
        model.add(Dropout(0.35))
        model.add(LSTM(64, return_sequences=False))
        model.add(Dropout(0.3))
        model.add(Dense(25, activation='relu'))
        model.add(Dense(1))

        # Compile the model. We use MSE because it penaltizes big errors, which is what we want to avoid
        model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])

        # Ð¢rain the model
        model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs)

        model.save('gym_hpa/predictions/pred_models/lstm_models/local_trained')
        # Structure of the model
        #keras.utils.plot_model(model, 'multi_input_and_output_model.png', show_shapes=True)

        return model

    def lstm_test(self, model, x_test, scaler):
        """
        Implementation of the testing for the LSTM model - makes the predictions for the testset.

        :param model: The trained LSTM model
        :param x_test: The normalized values of the testset
        :param scaler: The scaler value
        :return: The predictions for every observation in the testset
        """
        # Predict on test data
        lstm_predictions = model.predict(x_test, verbose=1)
        lstm_predictions = scaler.inverse_transform(lstm_predictions)
        # lstm_predictions = np.vstack((self.testset.filter([self.val_col]).values.astype(int), lstm_predictions.astype(int)))
        return lstm_predictions

    def naive(self):
        """
        Implementation of the naive method in-sample. The forecast for the next value is the current one. It is used as
        a benchmark. In very random environments, it often outperforms more complex models.

        :return: the predictions for every value of the dataset
        """
        testset = self.testset.filter([self.val_col]).values
        naive_predictions = np.roll(testset, 1)
        naive_predictions[0] = naive_predictions[1]
        return naive_predictions

    def naive_dynamic(self):
        """
        Implementation of the naive method out-of-sample. The forecast for the next value is the current one. It is used as
        a benchmark. In very random environments, it often outperforms more complex models.

        :return: the prediction for the next value
        """
        return self.testset.filter([self.val_col]).values[-1]

    def ses(self, alpha):
        """
        Implementation of the SES (Simple Exponential Smoothing) algorithm in-sample.

        :param alpha:
        :return: The predicted values for all the observations in the testset.
        """
        timeseries = self.testset.filter([self.val_col]).values
        forecast = []
        forecast.append(int(np.mean(timeseries)))  # calculate the average to be the starting value for the forecast

        for t in range(len(timeseries)):
            forecast.append(int(alpha * timeseries[t] + (1 - alpha) * forecast[t]))
        return forecast[:-1]

    def ses_dynamic(self, alpha=0.4):
        """
        Implementation of the SES (Simple Exponential Smoothing) algorithm out-of-sample.

        :param alpha: The 'alpha' parameter of the SES algorithm
        :return: The predicted value of the next observation
        """
        forecast = int(alpha * self.current_value + (1 - alpha) * self.testset.filter([self.val_col+'_predictions']).values[-1])
        return forecast

    def arima_fit(self):
        """
        Specifies the parameters of the ARIMA model and fits it into the testset
        :return: The fitted ARIMA model
        """
        testset = self.testset[self.val_col].values
        stepwise_fit = auto_arima(testset, start_p=0, start_d =0, start_q=0,
                          max_p=5, max_d=5, max_q=5, seasonal=False, trace = False,
                          supress_warnings=True)
        p, d, q = stepwise_fit.order
        # Fit ARIMA model with optimal parameters
        model = ARIMA(testset, order=(p, d, q))
        fitted_model = model.fit()
        return fitted_model

    def arima(self, model):
        """
        Applies in_sample predictions for the observations in the
        :param model:
        :return:
        """
        forecast = model.predict()
        return forecast

    def arima_dynamic(self, model):
        """
        Applies out-of-sample prediction for the last observation using an ARIMA model.

        :param model: The ARIMA model
        :return: The forecast for the ARIMA model (with a prediction horizon of self.n_steps)
        """
        forecast = model.forecast(steps=self.n_steps)

        return forecast


    def lstm(self, batch_size, epochs):
        """
        A function that executes both the training and the testing for the given datasets.

        :param batch_size: The batch size of the training
        :param epochs: The number of epochs to train
        :return: The predictions for each observation in the testing dataset
        """
        x_train, y_train, scaler_train = self.create_env_for_NN(self.trainset)
        x_test, y_test, scaler_test = self.create_env_for_NN(self.testset)
        print('Model training...')
        model = self.lstm_train(x_train, y_train, batch_size, epochs)
        print('Model testing...')
        lstm_predictions = self.lstm_test(model, x_test, scaler_test)
        return lstm_predictions

    def lstm_test_dynamic(self, model):
        """
        Applies out-of-sample prediction for the last obserrvation using lstm prediction method.

        :param model: The trained lstm model used for predictions
        :return: the prediction for the last observation
        """
        x_test, y_test, scaler_test = self.create_env_for_NN(self.testset)
        # print(x_test.shape)
        lstm_predictions = self.lstm_test(model, x_test, scaler_test)
        return lstm_predictions[-1]

    def average(self, prediction: dict[str, list] = None):
        """
        Implementation of the average of many independent prediction algorithms in-sample.
        It is a more moderate approach, usually wielding more accurate results.

        :param prediction: optional - a dictionary containing the predictions formed by the algorithms.
                           The key must be a string and the value a list containing floats.
                           If not provided, the naive/ses(0.4)/arima are used. You can change them in the code.
        :return: the predictions for every value of the dataset
        """
        if prediction is None:
            prediction = {}

            prediction['Naive'] = self.naive()
            prediction['SES'] = self.ses(0.4)
            # add more if you want to

        return np.mean(prediction.values(), axis=0)

    def average_dynamic(self, prediction: dict[str, float] = None):
        """
        Implementation of the average of many independent prediction algorithms out-of-sample.
        It is a more moderate approach, usually wielding more accurate results.

        :param prediction: optional - a dictionary containing the predictions formed by the algorithms.
                           The key must be a string and the value a float.
                           If not provided, the naive/ses(0.4)/arima are used. You can change them in the code.
        :return: the predictions for every value of the dataset
        """
        if prediction is not None:
            prediction = {}

            prediction['Naive'] = self.naive_dynamic()
            prediction['SES'] = self.ses_dynamic(0.4)
            # add more if you want to

        return np.mean(prediction.values(), axis=0)

    def rmse(self):
        """
        Calculation of the RMSE (Root Mean Squared Error) for the predictions of the testset.
        The lower the value, the higher the accuracy.
        It is expressed in the same units as the values in the dataset.
        Gives no information about the bias of the algorithm.
        It emphasises larger errors.

        :return: The calculated RMSE
        """
        RMSE = np.sqrt(np.mean(((self.testset[self.val_col+'_predictions'] - self.testset[self.val_col]) ** 2)))

        return RMSE

    def mae(self):
        """
        Calculation of the MAE (Mean Absolute Error) for the predictions of the testset.
        The lower the value, the higher the accuracy.
        It is expressed in the same units as the values in the dataset.
        Gives no information about the bias of the algorithm.

        :return: The calculated MAE
        """
        MAE = np.mean(np.abs(self.testset[self.val_col] - self.testset[self.val_col+'_predictions']))
        return MAE

    def mape(self):
        """
        Calculation of the MAPE for the predictions of the testset.
        The lower the value, the higher the accuracy.
        It is expressed in a percentage (%).
        Gives no information about the bias of the algorithm. It is the normalized version of the MAE.
        Because it is normalized it is more suitable for comparing the accuracy of an algorithm in different datasets.

        :return: The calculated MAPE
        """
        MAPE = np.mean(np.abs((self.testset[self.val_col] - self.testset[self.val_col+'_predictions']) / self.testset[self.val_col])) * 100
        return MAPE

    def me(self):
        """
        Calculation of the ME (Mean Error) for the predictions of the testset.
        The lower the value, the higher the accuracy.
        It is expressed in the same units as the values in the dataset.
        Negative values indicate that the algorithm tends to overshoot (predictions usually higher than the real values)
        Positive values indicate that the algorithm tends to undershoot (predictions usually lower than the real values)

        :return: The calculated ME
        """
        ME = np.mean(self.testset[self.val_col] - self.testset[self.val_col + '_predictions'])
        return ME


